{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import sqlite3\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from fake_useragent import UserAgent\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de la base de datos\n",
    "con = sqlite3.connect('anagrama.db')\n",
    "cursorANA = con.cursor()\n",
    "cursorANA.execute(\n",
    "    '''\n",
    "    CREATE TABLE IF NOT EXISTS anagrama\n",
    "    (\n",
    "    isbn VARCHAR,\n",
    "    ean INT,\n",
    "    código VARCHAR,\n",
    "    título TEXT,\n",
    "    autor TEXT,\n",
    "    precio FLOAT,\n",
    "    páginas INT,\n",
    "    fecha_publicación DATE,\n",
    "    colección TEXT,\n",
    "    traducción TEXT,\n",
    "    sinopsis TEXT,\n",
    "    ebook BOOLEAN INT\n",
    "    );\n",
    "    ''')\n",
    "con.commit()\n",
    "con.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creación de un directorio para almacenar las imágenes\n",
    "if not os.path.exists('portadas'):\n",
    "    os.mkdir('portadas')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Esqueleto del scraper\n",
    "\n",
    "def scrap_anagrama(proportional_delay=True):\n",
    "    \n",
    "    def get_links_colecciones():\n",
    "        '''\n",
    "        Devuelve diccionario que contiene las colecciones\n",
    "        de la editorial Anagrama\n",
    "        '''\n",
    "        html = requests.get(url_colecciones, headers).content\n",
    "        soup = BeautifulSoup(html)\n",
    "        colecciones = {}\n",
    "        for link in soup.findAll('a'):\n",
    "            coleccion_link = url_anagrama + str(link.get('href'))\n",
    "            coleccion = coleccion_link.rsplit('/', 1)[-1].replace('-',' ')\n",
    "            if \"/coleccion/\" in coleccion_link:\n",
    "                colecciones[coleccion] = coleccion_link\n",
    "        del colecciones['ebooks']\n",
    "        return colecciones\n",
    "\n",
    "\n",
    "    def get_links_libros_in_page(url_page):\n",
    "        '''\n",
    "        Devuelve una lista que contiene los libros\n",
    "        de una página del sitio web\n",
    "        '''\n",
    "        html = requests.get(url_page, headers).content\n",
    "        soup = BeautifulSoup(html)\n",
    "        links_libros = set([link['href'] for link in soup.find_all(\n",
    "            \"a\", class_=\"book\", href=True) if link['href'].startswith('/libro/')])\n",
    "        return links_libros\n",
    "        \n",
    "\n",
    "    def get_libro_info(url_libro):\n",
    "        '''\n",
    "        Recoge la información básica del libro\n",
    "        dada una url de la editorial Anagrama\n",
    "        y lo carga en un diccionario\n",
    "        '''\n",
    "        url_libro = url_libro\n",
    "        html = requests.get(url_libro, headers).content\n",
    "        soup = BeautifulSoup(html)\n",
    "        \n",
    "        datos_libro = dict()\n",
    "        \n",
    "        # Básicos\n",
    "        datos_libro[\"TÍTULO\"] = soup.h1.get_text()\n",
    "        datos_libro[\"AUTOR/A\"] = soup.title.get_text().split('-')[1].strip()\n",
    "        datos_libro[\"RESUMEN\"] = soup.find('div', class_=\"textContent mt10px\").get_text()\n",
    "        \n",
    "        # Tabla\n",
    "        for e in soup.find('table'):\n",
    "            data = e.find_all('td')\n",
    "            col, row = data[0].get_text(), data[1].get_text()\n",
    "            datos_libro[col] = row\n",
    "        \n",
    "        # Ebook \n",
    "        datos_libro[\"EBOOK\"] = soup.find(\"div\", class_=\"tab-pane ebook\") != None\n",
    "        \n",
    "        # Corrección de la información de COLECCIÓN\n",
    "        datos_libro[\"COLECCIÓN\"] = url_libro.split('/')[4].replace('-',' ').title()\n",
    "        \n",
    "        # Portada libro\n",
    "        try: \n",
    "            datos_libro[\"URL PORTADA\"] = soup.find('meta', property=\"og:image\")['content']\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "        return datos_libro\n",
    "        \n",
    "\n",
    "    def get_row_values(info_libro):\n",
    "        '''\n",
    "        Devuelve una lista de la información de un libro en formato apto\n",
    "        para ser cargado en la base de datos\n",
    "        '''\n",
    "        columns = ['ISBN', 'EAN', 'TÍTULO', 'AUTOR/A', 'COLECCIÓN', 'CÓDIGO',\n",
    "                   'NÚM. DE PÁGINAS', 'PUBLICACIÓN', 'PVP CON IVA', 'RESUMEN',\n",
    "                   'TRADUCCIÓN', 'EBOOK']\n",
    "        for key, value in info_libro.items():\n",
    "            row_values = []\n",
    "            for column in columns:\n",
    "                try:\n",
    "                    row_values.append(info_libro[column])\n",
    "                except:\n",
    "                    row_values.append(None)\n",
    "        return row_values\n",
    "        \n",
    "\n",
    "    def sql_insert(con, row_values):\n",
    "        '''\n",
    "        Actualiza la base de datos con un nuevo registro\n",
    "        '''\n",
    "        cursorObj = con.cursor()\n",
    "        cursorObj.execute('''\n",
    "                INSERT INTO \n",
    "                anagrama(isbn, ean, título, autor, colección, código,\n",
    "                páginas, fecha_publicación, precio, sinopsis, traducción, ebook\n",
    "                ) \n",
    "                VALUES(?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                ''', row_values)\n",
    "        con.commit()\n",
    "        \n",
    "    \n",
    "    def load_requests(source_url, ean, coleccion):\n",
    "        '''\n",
    "        Descarga y almacena la portada del libro\n",
    "        '''\n",
    "        r = requests.get(source_url, headers=headers, stream=True)\n",
    "        if r.status_code == 200:\n",
    "            ruta = \"portadas\\\\\"+ean+'-'+coleccion+'.jpg'\n",
    "            output = open(ruta,\"wb\")\n",
    "            for chunk in r:\n",
    "                output.write(chunk)\n",
    "            output.close()\n",
    "    \n",
    "    \n",
    "    con = sqlite3.connect('anagrama.db')\n",
    "\n",
    "    url_anagrama = 'https://www.anagrama-ed.es'\n",
    "    url_colecciones = 'https://www.anagrama-ed.es/colecciones'\n",
    "    \n",
    "    headers = {\"User_Agent\":UserAgent().random}  # User agent generado aleatoriamente\n",
    "    \n",
    "    colecciones = get_links_colecciones()\n",
    "    \n",
    "    for coleccion, link in colecciones.items():\n",
    "        page = 1\n",
    "        while bool(page)==True:\n",
    "            url_page = link + '?p=' + str(page)\n",
    "            listado_links_libros = get_links_libros_in_page(url_page)\n",
    "            if len(listado_links_libros) != 0:\n",
    "                for url_libro in listado_links_libros:\n",
    "                    url_libro = url_anagrama + url_libro\n",
    "                    t0 = time.time()\n",
    "                    info_libro = get_libro_info(url_libro)\n",
    "                    response_delay = time.time() - t0\n",
    "                    row_values = get_row_values(info_libro)\n",
    "                    sql_insert(con, row_values)\n",
    "                    try:\n",
    "                        load_requests(info_libro['URL PORTADA'],\n",
    "                                     info_libro['EAN'],\n",
    "                                     info_libro['COLECCIÓN'])\n",
    "                    except: \n",
    "                        pass\n",
    "                    if proportional_delay==True: \n",
    "                        time.sleep(10*response_delay)  # Espaciado automático entre peticiones\n",
    "                page += 1\n",
    "            else:\n",
    "                page = False\n",
    "    return '¡Listo!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'¡Listo!'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Raspado del sitio web y generación de informe\n",
    "\n",
    "scrap_anagrama(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversión de la base de datos a csv\n",
    "\n",
    "conn = sqlite3.connect('anagrama.db', isolation_level=None,\n",
    "                      detect_types=sqlite3.PARSE_COLNAMES)\n",
    "\n",
    "df = pd.read_sql_query(\"SELECT * FROM anagrama\", conn)\n",
    "df.to_csv('database.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
